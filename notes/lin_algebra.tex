\documentclass{article}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}

\newcommand{\real}{\mathbb{R}}
\usepackage{verbatim}

\begin{document}
\setcounter{section}{4}
\section{Basic aspects of linear algebra and matrix computations}


\section{Linear Algebra and Regressions: Technical Aspects in C}
In this chapter we will see how basic linear algebra concepts will
serve us to create models to interpret a set of measured points. 

With basic linear algebra and matrix notation, we will be able to
estimate parameters in a model (i.e. by making least square fitting)
or explore data by using non parametric descriptions like Principal
Component Analysis. 

Before diving into direct applications we will start by 
understating what is the best way to use matrices in C and how to take
advantage of the GNU Scientific Library. 


\subsection{Matrices}
We will consider a matrix as a rectangular array of real (or complex)
numbers arranged in sets of $m$ rows with $n$ entries. The set of
matrices with real coefficients with size $m\times n$ is called
$\real^{m\times n}$. In the context of this chapter vectors will be
understood as a column vectors, that is as a matrix of one column
belonging to the set $\real^{m}$.  

We will refer to the $a_{ij}$ element of matrix ${\mathbf A}$ to
indicate the element located in row $i$ and column $j$. In the case of
a vector ${\mathbf x}$ we will refer to its components as $x_{i}$. 


\begin{displaymath}
\mathbf{A} = 
\left[
\begin{array}{cccc}
a_{11}  & a_{12}  & \dots & a_{1n}   \\
a_{21}  &  \ddots &  & \vdots  \\
\vdots  &  & \cdots & a_{mn}   
\end{array}
\right]
\qquad
\mathbf{x} = 
\left[
\begin{array}{c}
x_{1}     \\
x_2          \\
\vdots     \\
x_{m}   
\end{array}
\right]
\end{displaymath}


\subsection{Matrices as pointers in C}
Matrices in C are best stored as one dimensional arrays. This implies
that there is convention to translate the position $i,j$ in the matrix
into the index $p$ in the one dimensional array.  

This convention in C is called {\bf raw major order}, where the second
index moves continuously in memory. It means that the following
$3\times 3$ matrix: 


\begin{displaymath}
\mathbf{A} = 
\left[
\begin{array}{ccc}
a_{11}  & a_{12} & a_{13}   \\
a_{21}  & a_{22}   & a_{23}  \\
a_{31}  & a_{32} & a_{33}   
\end{array}
\right]
\end{displaymath}

will be stored in memory as the following one dimensional array

\begin{displaymath}
\mathbf{a} = 
\left[
\begin{array}{ccccccccc}
a_{11}  & a_{12} & a_{13}   & a_{21}  & a_{22}   & a_{23}  & a_{31}  & a_{32} & a_{33}   
\end{array}
\right]
\end{displaymath}.

In FORTRAN the convention is the opposite, the first index moves
continously in memory. This convention is known as {\bf column major
  order}.  

In the following example we define a matrix $5\times 5$ and initialize it to be 
the identity. 

\verbatiminput{../hands_on/lin_algebra/identity.c}


\subsection{Functions taking pointers in C}
We go into the details of dealing with a array/matrix operation in
C. Most of the operations we make onto a matrix will be done using
functions. Functions in C work with variables passed as reference, not
as values. That means that if you want to have a variable modified,
you have to give the address where the variable lives in memory. In
practice, that means that you should work only with pointers. 


In the next example we define a matrix $3\times3$ an take its transpose.

\verbatiminput{../hands_on/lin_algebra/transpose.c}

What would happen if you try to modify the variable \verb"n_x" inside a function?

\subsection{GNU Scientific Library}

Most of the computations that we will perform in C will be done using
external libraries. We will tap on the work done by other
people. However simple it may seem it is important that you understand
how external libraries can be used in C.  

\begin{verbatim}
sudo apt-get install libgsl0-dev
sudo apt-get install liblapack-dev
\end{verbatim}

Let's start by considering the following example that uses the gsl
library to compute the the Bessel function $J_{0}(x)$ for $x=5$: 

\begin{verbatim}
#include <stdio.h>
#include <gsl/gsl_sf_bessel.h>
     
int main (void){
       double x = 5.0;
       double y = gsl_sf_bessel_J0 (x);
       printf ("J0(%g) = %.18e\n", x, y);
       return 0;
}
\end{verbatim}

If the setup for the library are the standard ones, the program can be compiled as

\begin{verbatim}
cc -lgsl -lgslcblas -lm test.c 
\end{verbatim}



If you are under UBUNTU probably the compilation order matters and you
should instead write 

\begin{verbatim}
cc test.c -lgsl -lgslcblas -lm 
\end{verbatim}

Executing the program will produce the following results:

\begin{verbatim}
J0(5) = -1.775967713143382642e-01
\end{verbatim}.


GSL also has special libraries to deal with linear algebra operations. A detailed webpage including the manual and useful examples can be found here:

\begin{verbatim}
http://www.gnu.org/software/gsl/manual/html_node/
\end{verbatim}

Now try to follow the structure in the following program that takes a
square matrix and computes its eigenvalues\footnote{Example taken from
  the example manual of the GSL webpage.}): 


\verbatiminput{../hands_on/lin_algebra/eigen.c}

\section{Linear Algebra and Regressions: Mathematical Aspects}

We are now interested in giving a mathematical background to the
problem of regressions in physics\footnote{This section is based on
  German Prieto notes on inversion theory in geosciences.}.  
The following situation is very common in physics, we the measurement
of some quantity, say the spectra from a distant galaxy and we want
infer other physical properties in that galaxy (mass and age, for
instance). 

In general the methods to infer the properties of a model from the
measurements is known as inverse theory. The methods that allows us to
pick the most probable model from a series of measurements will be
called statistical inference. Both are related and have many
similarities. 

In general mathematical terms, if we have a model for a physical
phenomenon described by a set of parameter and we have a theory that
can relate those parameters to produce an observable, we can write 

\begin{equation}
d_{i} = G_i (\mathbf{m}),
\end{equation}

where ${\mathbf m}$ contains all the instances of the physical
parameters in my model (i.e time, masses, etc), $d$ is the observed
data and $G$ is the is the theory that produces the observed data $d$
from the parameters in the model $m$. The index $i$ goes over the
number of measurements that you have. 

Why is this problem hard? Because you might have a model described by
infinite number of parameters and a finite number of measurements. In
that case we talk about and undetermined problem. You might decide to
oversimplify the problem and reduce the number of parameters, if you
have more measurements than parameters in the model, you are in the
extreme of an overdetermined problem. 

This problem can be written as matrix multiplications: 


\begin{equation}
\mathbf{d} = \mathbf{G}\mathbf{m}
\end{equation}
where $\mathbf{d}$ represents the vector with the data. $\mathbf{G}$
is the theory that we use (electrodynamics, gravitation, etc) for
predicting data and $\mathbf{m}$ is the model that we wish to obtain. 


\subsection{Getting Practical: linear regressions}

The measurements that you have might be the positions as a function of
time of ball flying through the air. You decided that the physics that
best apply in that case are the kinematics of a movement with uniform
acceleration. Under that theory the model that describes the position
as a function of time is the following: 

\begin{equation}
y(t) = m_1 + m_2 t + (1/2) m_3 t^2
\end{equation}

where $y(t)$ represents the altitude of the object at time $t$, and
the three $(N=3$) model parameters $m_i$ are associated with a
constant, slope and quadratic terms. Note that even if the function is
quadratic, the problem in question is linear for the three parameters.  

If we have $M$ discrete measurements $y_i$ at times $t_i$, the linear
regression problem or inverse problem can be written in the form 
\begin{equation}
\left[
\begin{array}{c}
  y_1   \\
  y_2   \\
  \vdots \\
  y_M
\end{array}
\right] = 
\left[
\begin{array}{ccc}
  1 & t_1 & {\scriptstyle{\frac{1}{2}}}t_1^2 \\
  1  &t_2 &  {\scriptstyle{\frac{1}{2}}}t_2^2\\
  \vdots & \vdots  & \vdots\\
  1 & t_M &  {\scriptstyle{\frac{1}{2}}}t_M^2
\end{array}
\right]
\left[
\begin{array}{c}
  m_1   \\
  m_2    \\
  m_3
\end{array}
\right] 
\end{equation}
When the regression model is linear in the unknown parameters, then we
call this a linear regression or linear inverse problem.  

Solving this problem means finding a model $\mathbf{\hat{m}}$ that
best reproduced the observed values. \emph{Best reproducing} means
seeing how different are the observations $\mathbf{d}$ when compared
to the results of the theory computed over the model: $\mathbf{G}
\mathbf{\hat{m}}$. 

Suppose we are given a collection of $M$ measurements of a property to
form a vector $\mathbf{d} \in \real^M$. We know forward problem such
that we can predict the data from a known model $\mathbf{m} \in
\real^N$. That is, we know the $N$ vectors $\mathbf{g}_k \in \real^M$
such that   

\begin{equation}
\mathbf{d} = \sum_{k=1}^{N} \mathbf{g}_k m_k = \mathbf{G} \mathbf{m}
\end{equation}
where
\begin{equation}
\mathbf{G} = \left[ \mathbf{g}_1, \mathbf{g}_2, \dots, \mathbf{g}_N \right] 
\end{equation}

We are looking for a model $\hat{\mathbf{m}}$ that minimizes the size
of the residual vector defined 

\begin{equation}
\mathbf{r} = \mathbf{d} -  \sum_{k=1}^{N} \mathbf{g}_k \hat{m}_k
\end{equation}

We do not expect to have an exact fit so there will be some error, and
we use a norm to measure the size of the residual 
\begin{equation}
\| \mathbf{r} \| = \| \mathbf{d} - \mathbf{Gm} \|
\end{equation}

For the least squares problem we use the $L2$ or Euclidean norm 
\begin{equation}
\| \mathbf{r} \|_2 = \sqrt{\sum_{k=1}^M r^2_k}
\end{equation}

Minimizing this quantity will give you the best parameters of your model.


We won't derive the results here, but assuming the inverse of $(\mathbf{G}^T \mathbf{G})$ exists, we can isolate $\hat{\mathbf{m}}$ to end up with

\begin{equation}
\mathbf{G}^T \mathbf{G}\hat{\mathbf{m}} = \mathbf{G}^T \mathbf{d}   
\label{eq:mlsq}
\end{equation}

Note that the matrix ${(\mathbf{G}^T \mathbf{G})}$ is a square
$N\times N$ matrix and $\mathbf{G}^T\mathbf{d}$ is an $N$ column
vector.  

If we express Equation (\ref{eq:mlsq}) as
$\mathbf{A}=\mathbf{G}^{T}\mathbf{G}$ and $\mathbf{G}^T\mathbf{d} =
\mathbf{b}$, everything becomes a linear problem
(i.e. $\mathbf{A}\hat{\mathbf{m}}=\mathbf{b}$) that begs to be
solved using LU decomposition or Cholesky decomposition.

\subsection{Getting Practical: Principal Component Analysis}

Many times in physics you don't have a theory, not event a model, of the data your are getting. However, in the process of building a model you would like to explore your measurements and get a handle on the physical process that might affect your data.

One of the first techniques that you have to learn in statistical analysis is Principal Component Analysis (PCA). The objective of this section is that you learn to perform and interpret data using this technique.

This time we start from $M$ different measurements of points in $N$ dimensions. The dimensions might be different physical quantities (velocities, masses), or might be the points in wavelength where you measure the intensity of a spectrum.

We start by measuring the covariance matrix, ${\bf\Sigma}$ of different dimension of all the data points. This covariance matrix will be a $N\times N$ matrix where each element is computed as:

\begin{equation}
\sigma_{i,j} = \frac{\sum_{k=1}^{M} (d^{k}_i - \bar{d_i})(d^{k}_j -\bar{d_{j}})}{M-1}
\end{equation}

In this notation there are $M$ vector of measurements ${\mathbf d}^{1}$,\ldots,${\mathbf d}^{M}$. Where each vector has $N$ components, for instance the $k$-th measurement will be ${\mathbf d}^{k}=(d^k_{1},\ldots,d^k_{N})$.

Some properties of the covariance matrix are the following:
\begin{enumerate}
\item The matrix ${\bf\Sigma}$ is symmetric;
  $\sigma_{ij}=\sigma_{ji}$.
\item Diagonal elements correspond to the the variance of the
  corresponding variables $\sigma_{ii}=\sigma_i^2$.
\item The covariance matrix is defined as non-negative
  ($\sigma_{ij}\geq 0$).
\end{enumerate}

The value of each entry in the covariance matrix depends on the units
defined to make the measurements. An alternative is to use the
correlation matrix,${\mathbf R}$, which normalizes each measuremente
by the square root of the variance along that dimension. As a result
the diagonal elements of ${\mathbf R}$ are unity $\rho_{ii}=1$.

The PCA technique consists in calculating the covariance matrix (or
the correlation matrix to avoid the arbitrary unit scaling) and
calculating its eigenvalues and eigenvectors. If you rank the
eigenvalues in descending order the associated eigenvectors will tell
the directions (in $N$ dimensional space) where the variance
decreases. With this you will be able to reduce the dimensionality of
your problem and express each point as the sum of the eigenvectors of
this matrix.

As a more concrete example of the application of Principal Component
Analysis let's take some data from the context of neurosciences. We
have a patient and we are measuring her brain activity through
encephalograms taken with 24 electrodes at different points on its
head. The encephalogram measures electric activity as a function of
time. In this case we have now 24 different signals taken at 400
different times. In the figure we show two different signals.

\end{document}
